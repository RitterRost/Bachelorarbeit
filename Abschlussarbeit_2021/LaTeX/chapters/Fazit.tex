\chapter{Conclusion}

In the first part of the work, the experimental part, the double Descent phenomena is first verified and then the influence of certain parameters and architectures is investigated. For a more detailed description, what kind of influence which parameter has, we refer to the discussion in chapter \ref{Discussion}.  However, it can be summarized that the phenomenon occurs under different optimization algorithms and network architectures. \\
 is the shape of the curve predictable? This question from the introduction can be answered with "yes". If the parameters are known, chapter \ref{experimental_part} shows that the resulting curve has certain predictable properties.\\ 
 In the second chapter, the behavior of the curve is analyzed in more detail. For this purpose, two sections of the double descent risk curve are examined individually. The two sections in question are called the underparameterized region and the overparameterized region. They are separated by the interpolation threshold, from which training data are completely interpolated. \\
A justification for the behavior of the curve in the underparameterized region is provided by the bias variance trade-off. It explains how the error term to be minimized depends on bias and variance. The dependence derived in the chapter (\ref{bias_variance_eq}) provides the shape of the first part of the curve. \\
For the second regime of the double descent risk curve, the overparameterized region, such a precise explanation for the behavior is missing. Nevertheless, assumptions can be made on the basis of observations on performed experiments. \\
The said observations occupy a large section of the thesis. First, learned functions are examined in dependence of the network size. It turns out that more parameters in models lead to a smoother learned function. larger neural networks have a feel for the distribution of the data and therefore generalize better.
It is also shown that average weight changes and epochs needed to find a minimum that ensures that the training data set is interpolated decreases with increasing parameter number. It can be observed that the decrease is proportional to the generalization curve, i.e. the test loss. It can therefore be assumed that larger networks find it much easier to interpolate the training data.\\
Another important finding is that the decomposition of the error from bias and variance (\ref{bias_variance_eq}) must also hold in the overparameterized domain. The conclusion from this is that the variance must fall monotonically in the overparameterized range. This is a finding which fits well with the observations just described. Thus, simpler and simpler functions can no longer vary so strongly. A smaller average weight change as well as a smaller number of required epochs also suggests that learned functions do not vary so much among themselves.\\
Following the observations made, two theories are presented as possible explanations for the monotonic decrease of the curve in the overparameterized region. The first theory states that larger networks have a self-regulatory influence on the training process. Thus, despite interpolation of the training data, weaker overfitting occurs. This theory is also consistent with the observations made. Thus, smaller weight changes and a lower variance have a regularizing influence.\\
The second theory is called the "winning lottery hypothesis". Here it is assumed that there are subgraphs in network graphs which allow a good convergence of SGD by their initialization. These subgraphs are called "winning tickets". A linear increase of the number of parameters causes an exponential increase of the number of subnetworks. The probability of a well initialized subnetwork thus increases. \\
The question from the beginning of the thesis, how the drop of the double descent curve can be explained, cannot be answered exactly. However, the idea why the test loss decreases with increasing number of parameters has become tangible after the thesis. It is also plausible why the performance does not increase after the interpolation threshold. This is due to the fact that data can be interpolated with different ellegance. \\
Chapter \ref{train_dd} shows interesting effects that can occur during training due to the double descent phenomenon. A seemingly paradoxical finding is that double descent can lead to poorer performance, for a given model size, with more data. Performance can be improved by taking into account double descent. \\

Chapter \ref{train_dd} has already shown us the importance of the double descent risk curve. But not only for reasons of better performance of the networks the phenomenon should be investigated further. It also gives us further understanding to a technique, the neural networks, whose behavior still cannot be completely explained. Especially with the more and more relevant topic of artificial intelligence in our everyday life. Exploring the functionality of neural networks is therefore an important aspect.
More and more papers dealing with this topic confirm this. The high number of published papers also shows how broad and versatile this topic is. Thus, this thesis could cite many more papers. But this would be beyond the scope of this work.  Even so, parts of this work were only superficially touched. For example, it would be interesting to further investigate the loss landscape of overparameterized networks, since the use of the SAM optimizer has produced an interesting generalization curve. \\
Proving the lottery ticket hypothesis would also be an interesting topic for further work. After all, this hypothesis has already yielded astonishing results. \\
It would also be interesting to know how many minimums exist depending on the number of parameters that interpolate the training data. Thus, further insights into the self regularizing effect of larger networks could be gained. \\
So there is still much to speculate on the subject of double descent and it can be eagerly awaited what knowledge will be gained about it in the future.








