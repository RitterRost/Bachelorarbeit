{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3952,"status":"ok","timestamp":1649256758067,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"},"user_tz":-120},"id":"7ZmwTMWx9AcY"},"outputs":[],"source":["#imports \n","\n","%tensorflow_version 2.x\n","import tensorflow as tf \n","from tensorflow import keras \n","\n","#Help-liberies \n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","source":["testing the SAM Optimizer"],"metadata":{"id":"0Bc8EQVhBlxe"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"IavwwvkdKA-m","executionInfo":{"status":"ok","timestamp":1649256881600,"user_tz":-120,"elapsed":305,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"}}},"outputs":[],"source":["class SAM():\n","    def __init__(self, base_optimizer, rho=0.01, eps=1e-12):\n","        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n","        \n","        self.rho = rho\n","        self.eps = eps\n","        self.base_optimizer = base_optimizer\n","\n","    def first_step(self, gradients, trainable_vars):\n","        self.e_ws = []\n","        grad_norm = tf.linalg.global_norm(gradients)\n","        ew_multiplier = self.rho / (grad_norm + self.eps)\n","        for i in range(len(trainable_vars)):\n","            e_w = tf.math.multiply(gradients[i], ew_multiplier)\n","            trainable_vars[i].assign_add(e_w)\n","            self.e_ws.append(e_w)\n","\n","    def second_step(self, gradients, trainable_variables):\n","        for i in range(len(trainable_variables)):\n","            trainable_variables[i].assign_add(-self.e_ws[i])\n","        # do the actual \"sharpness-aware\" update\n","        self.base_optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","\n","# if you want to use model.fit(), override the train_step method of a model with this function, example is mnist_example_keras_fit.\n","# for customization see https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/\n","def sam_train_step(self, data, rho=0.01, eps=1e-12):\n","    # Unpack the data. Its structure depends on your model and\n","    # on what you pass to `fit()`.\n","    if len(data) == 3:\n","        x, y, sample_weight = data\n","    else:\n","        sample_weight = None\n","        x, y = data\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self(x, training=True)  # Forward pass\n","        # Compute the loss value\n","        # (the loss function is configured in `compile()`)\n","        loss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n","\n","    # Compute gradients\n","    trainable_vars = self.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","\n","    # first step\n","    e_ws = []\n","    grad_norm = tf.linalg.global_norm(gradients)\n","    ew_multiplier = rho / (grad_norm + eps)\n","    for i in range(len(trainable_vars)):\n","        e_w = tf.math.multiply(gradients[i], ew_multiplier)\n","        trainable_vars[i].assign_add(e_w)\n","        e_ws.append(e_w)\n","\n","    with tf.GradientTape() as tape:\n","        y_pred = self(x, training=True)  # Forward pass\n","        # Compute the loss value\n","        # (the loss function is configured in `compile()`)\n","        loss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)\n","        \n","    trainable_vars = self.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","\n","    for i in range(len(trainable_vars)):\n","        trainable_vars[i].assign_sub(e_ws[i])\n","    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","    # Update the metrics.\n","    # Metrics are configured in `compile()`.\n","    self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n","\n","    # Return a dict mapping metric names to current value.\n","    # Note that it will include the loss (tracked in self.metrics).\n","    return {m.name: m.result() for m in self.metrics}"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pwiYmWF29Kh2","executionInfo":{"status":"ok","timestamp":1649256779139,"user_tz":-120,"elapsed":372,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"}}},"outputs":[],"source":["#The function takes the train labels and the amount of noise we want to apply \n","#The return value is the train_labels with noise \n","import random\n","\n","def adding_noise(labels, percent):\n","  labels_with_noise = np.zeros((len(labels)))\n","  for i in range(len(labels)):\n","    r = random.random()\n","    if r < percent:\n","      labels_with_noise[i] = (int(labels[i] + 1+ 9*random.random())%10)\n","    else:\n","      labels_with_noise[i] = labels[i] \n","  return np.array(labels_with_noise)"]},{"cell_type":"code","source":[""],"metadata":{"id":"xIlL28Gvjnue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CR7pt0n-IKt"},"source":["Creating the Class Model, where modelscan be created, trained and observed\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6xq2wEh697S-","executionInfo":{"status":"ok","timestamp":1649256780679,"user_tz":-120,"elapsed":4,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"}}},"outputs":[],"source":["\n","\n","class Model:\n","      model = keras.Sequential()\n","\n","      def __init__(self, layers, layersize):\n","\n","        self.layers = layers\n","        self.layersize = layersize\n","        #initializing the model\n","        self.model = keras.Sequential()\n","        self.model.add(keras.layers.Flatten(input_shape=(28,28)))#input layer\n","        for j in range(0, layers):\n","          self.model.add(keras.layers.Dense(layersize, activation=\"relu\",kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.05),bias_constraint=tf.keras.constraints.MinMaxNorm(\n","min_value=0.0, max_value=0.0, rate=1.0, axis=0))) #hidden layers\n","        self.model.add(keras.layers.Dense(10,activation='softmax')) #output layer\n","        #compile model\n","        self.model.compile(optimizer= tf.keras.optimizers.SGD(\n","    learning_rate=0.1),loss= 'sparse_categorical_crossentropy',metrics=['accuracy'])  \n","        self.model.count_params()\n","\n","      '''\n","      trains the model and returns a List of various data\n","      data[0] = the train loss after fitting the data \n","      data[1] = the train accuracy after fitting the data  \n","      data[2] = the test loss after evaluating the model \n","      data[3] = the test accuracy after evaluating the model  \n","      '''\n","\n","      def train(self,epoch,datasize,train_images,train_labels,test_images, test_labels):\n","        data = np.zeros(5)\n","        print(datasize,epoch)\n","\n","        #trainig the model until a certain accuracy is reached\n","        runs = 0\n","        train_loss = 100\n","        train_acc = 0\n","        e_per_run = 1\n","        while (train_loss > 0.01) and runs < epoch:\n","          history = self.model.fit(train_images[:datasize], train_labels[:datasize],epochs = e_per_run)\n","          train_loss_all, train_acc_all = history.history.values()\n","          train_loss = train_loss_all[e_per_run-1]\n","          train_acc = train_acc_all[e_per_run-1]\n","          runs = runs + e_per_run\n","\n","        test_loss, test_acc = self.model.evaluate(test_images, test_labels, verbose=1)\n","        data[0] = train_loss\n","        data[1] = train_acc\n","        data[2] = test_loss\n","        data[3] = test_acc\n","        data[4] = runs\n","        print(self.model.summary())\n","        return data\n","\n","      def getParameter(self):\n","        return self.model.count_params()\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"nIA3dy4h-uOM","executionInfo":{"status":"ok","timestamp":1649256783211,"user_tz":-120,"elapsed":3,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"}}},"outputs":[],"source":["\n","\n","\n","\n","def trainModel(amount,min_layersize,max_layersize, layers, epochs, datasize, noise):\n","  a = max_layersize - min_layersize \n","  data = np.zeros((8,a))\n","\n","  #load dataset MNIST\n","  mnist = keras.datasets.mnist\n","  #spliting into testing and training\n","  (train_images, train_labels_without_noise), (test_images, test_labels) = mnist.load_data()\n","  #normalizing data\n","  train_images = train_images/255.0\n","  test_images = test_images/255.0\n","  #adding noise\n","  train_labels = adding_noise(train_labels_without_noise, noise)\n","\n","  \n","  '''\n","  data[0][i] = the train loss after fitting the data in iterration i (with i layers) \n","  data[1][i] = the train accuracy after fitting the data in iterration i (with i layers)  \n","  data[2][i] = the test loss after evaluating the model in iterration i (with i layers) \n","  data[3][i] = the test accuracy after evaluating the model in iterration i (with i layers) \n","  data[4][i] = epochs needed for reachen train-risk < 0.01 in iterration i (with i layers)\n","  data[5][i] = average weight change in itteration i (with i layers)\n","  data[6][i] = average bias change in itteration i (with i layers)\n","  '''\n","\n","  for i in range(0,max_layersize - min_layersize):\n","    data_help = np.zeros(8)\n","    for j in range(0,amount):\n","      print(\"I an currently in run \",i, \" ===> The model has \", i+ min_layersize ,\"neurons. \")\n","      print(\"amount =\", amount) \n","      myModel = Model(layers,i + min_layersize)\n","\n","      #getting the weight-Vectors\n","      layer_weights_bevor = myModel.model.layers[1].get_weights()[0]\n","      layer_biases_bevor  = myModel.model.layers[1].get_weights()[1]\n","\n","      history = myModel.train(epochs, datasize,train_images,train_labels,test_images, test_labels)\n","      param   = myModel.getParameter()\n","\n","      #getting the weight-Vectors\n","      layer_weights = myModel.model.layers[1].get_weights()[0]\n","      layer_biases  = myModel.model.layers[1].get_weights()[1]\n","      \n","      \n","      #weight change\n","      print(\"Weightchange: \" ,np.shape(abs(layer_weights_bevor) - abs(layer_weights)))\n","      change_weights = abs((layer_weights_bevor) - (layer_weights)).flatten() \n","      change_biases = abs((layer_biases_bevor) - (layer_biases)).flatten()\n","\n","      average_weights_change = sum(change_weights)/len(change_weights)\n","      average_biases_change = sum(change_biases)/len(change_biases)\n","      print(average_weights_change)\n","\n","\n","\n","\n","      data_help[0] = data_help[0] + history[0] \n","      data_help[1] = data_help[1] + history[1]\n","      data_help[2] = data_help[2] + history[2]\n","      data_help[3] = data_help[3] + history[3]\n","      data_help[4] = data_help[4] + history[4]\n","      data_help[5] = data_help[5] + average_weights_change\n","      data_help[6] = data_help[6] + average_biases_change\n","      \n","\n","    #maby filtering runs, that went wrong\n","    data[0][i] = data_help[0]/amount\n","    data[1][i] = data_help[1]/amount\n","    data[2][i] = data_help[2]/amount\n","    data[3][i] = data_help[3]/amount\n","    data[4][i] = data_help[4]/amount\n","    data[5][i] = data_help[5]/amount\n","    data[6][i] = data_help[6]/amount\n","  return data "]},{"cell_type":"markdown","metadata":{"id":"UTPtKBXn-i_U"},"source":["in the next block, i want to creata a example curve of the double descent"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"p7S5NL0X8RQR","executionInfo":{"status":"error","timestamp":1649256974945,"user_tz":-120,"elapsed":2159,"user":{"displayName":"Ritter Rost","userId":"08820756572124446994"}},"outputId":"96776c44-3ab7-4624-f9c2-78b4251ad8b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["I an currently in run  0  ===> The model has  79 neurons. \n","amount = 1\n","15000 200\n","313/469 [===================>..........] - ETA: 0s - loss: 0.5731 - accuracy: 0.8419"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-5022580f7d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mplot__epochs_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-5022580f7d2d>\u001b[0m in \u001b[0;36mplot__epochs_needed\u001b[0;34m(max_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmin_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmax_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mdata_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'blue'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_epochs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-68de4634d3d4>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(amount, min_layersize, max_layersize, layers, epochs, datasize, noise)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mlayer_biases_bevor\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mparam\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-bd86b7db7424>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, datasize, train_images, train_labels, test_images, test_labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0me_per_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mruns\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdatasize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdatasize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_per_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m           \u001b[0mtrain_loss_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_per_run\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \"\"\"\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_caching_device\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_caching_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n","from matplotlib import cm\n","\n","def plot__epochs_needed(max_epochs):\n","  min_layer = 79\n","  max_layer = 80\n","  data = trainModel(1,min_layer,max_layer,1, max_epochs ,15000, 0) \n","  data_epochs = data[4]\n","  colors = ['red' if x == max_epochs else 'blue' for x in data_epochs]\n","  \n","\n","  fig, [ax1,ax2] = plt.subplots(2)\n","\n","  par1 = ax1.twinx()\n","  par2 = ax2.twinx()\n","\n","  fig.set_size_inches(20,30)\n","  ax1.bar(np.linspace(min_layer,(max_layer-1),(max_layer) - (min_layer)), data_epochs, color = colors , width = 0.8)\n","  ax1.set_xlabel('Layersize')\n","  ax1.set_ylabel('Amount of epochs needed')\n","  par1.plot(np.linspace(min_layer,(max_layer-1),(max_layer) - (min_layer)),data[2], color = 'black', label = 'test loss', linewidth= 4.0)\n","  par1.set_ylabel('train loss')\n","\n","  ax2.bar(np.linspace(min_layer,(max_layer-1),(max_layer) - (min_layer)), data[5], color = colors , width = 0.8)\n","  ax2.set_xlabel('Layersize')\n","  ax2.set_ylabel('Average weight-change')\n","  par2.plot(np.linspace(min_layer,(max_layer-1),(max_layer) - (min_layer)),data[2], color = 'black', label = 'test loss', linewidth= 4.0)\n","  par2.set_ylabel('train loss')\n","\n","\n","  #legend\n","  import matplotlib.patches as mpatches\n","\n","  pop_a = mpatches.Patch(color='red', label=\"0 test-loss not reached after \" + str(max_epochs) + \" epochs\")\n","  pop_b = mpatches.Patch(color='blue', label=\"0 test-loss reached after \" + str(max_epochs) + \" epochs\")\n","\n","  ax1.legend(handles=[pop_a,pop_b])\n","  ax2.legend(handles=[pop_a,pop_b])\n","   \n","  plt.savefig(\"kurve_with_various_epochs.png\")\n","\n","\n","plot__epochs_needed(200)\n"]},{"cell_type":"markdown","source":["We now want to estemate the desity-functions of the change of the weights\n","We observed a decreasing average weightchange over time. It would be nice to observe, that the network does not change some weights at all. "],"metadata":{"id":"sWwlVYJwvF3L"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5ouVK_uCaHz"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"eGl3yEau0swg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"SAM_Double_Descent_epochs_needed_Bachelorarbeit.ipynb","provenance":[{"file_id":"1GpbqqkLjfr3KjpYj1CYFaAqodC-gx5o_","timestamp":1648915734402},{"file_id":"1SSPWZ2pSC_ylsthegmagsLxluMGk-CrV","timestamp":1647441360663},{"file_id":"15KsO4F_j6p9Zo9Gv4t0fW7Afp32HvZzh","timestamp":1647182054327}],"authorship_tag":"ABX9TyO5l9Xk2U3dM58s/Hs6gZWx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}