{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3476,"status":"ok","timestamp":1646564769449,"user":{"displayName":"Ritter Rost","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08820756572124446994"},"user_tz":-60},"id":"Lh-B1URjhX89"},"outputs":[],"source":["#imports \n","\n","%tensorflow_version 2.x\n","import tensorflow as tf \n","from tensorflow import keras \n","\n","#Help-liberies \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","import random \n","\n","\n","\n","#we have 10 possible outcomes \n","class_names = ['0','1','2','3','4','5','6','7','8','9']"]},{"cell_type":"markdown","metadata":{"id":"fqr2sLSDwU0p"},"source":["#Nois on MNIST\n","For the experiment i want to put some noice over the Labels/\n","The idea is to misclassify some labels on purpose "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sxNoeTtLwHxC","executionInfo":{"status":"ok","timestamp":1646564773246,"user_tz":-60,"elapsed":434,"user":{"displayName":"Ritter Rost","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08820756572124446994"}}},"outputs":[],"source":["#The function takes the train labels and the amount of noise we want to apply \n","#The return value is the train_labels with noise \n","import random\n","\n","def adding_noise(y_labels, percent):\n","  for y in y_labels:\n","    r = random.random()\n","    if r < percent:\n","      y = int(y + 1+ 9*random.random())%10\n","  return y_labels"]},{"cell_type":"markdown","metadata":{"id":"2FM8dI_SiOSN"},"source":["#Creating a class Model with various attributes.\n","\n","- the attribute layers describes how many hidden-layers the network whitch gets created has. \\\n","- the attribute layersize describes the amount of neurons a hidden-layer has. \\\n","- epochs describes how many epochs get fit into the model\n","- autoeochs is a boolean which, if it's true, regulates the epochs which are nececarry for the model to perfom well\n","- amount stands for the amount of itteration a model performs. So the average can be calculated \n","- datasize is an integer which defines the size of the subset of the dataset   "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"LG1k1L5Hh1xU","executionInfo":{"status":"ok","timestamp":1646564778478,"user_tz":-60,"elapsed":1062,"user":{"displayName":"Ritter Rost","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08820756572124446994"}}},"outputs":[],"source":["[ ] \n","\n","\n","\n","class Model:\n","      model = keras.Sequential()\n","\n","      def __init__(self, layers, layersize):\n","\n","        self.layers = layers\n","        self.layersize = layersize\n","        #initializing the model\n","        self.model = keras.Sequential()\n","        self.model.add(keras.layers.Flatten(input_shape=(28,28)))#input layer\n","        for j in range(0, layers):\n","          self.model.add(keras.layers.Dense(layersize, activation=\"relu\")) #hidden layers\n","        self.model.add(keras.layers.Dense(10,activation='softmax')) #output layer\n","        #compile model\n","        self.model.compile(optimizer= 'adam',loss= 'sparse_categorical_crossentropy',metrics=['accuracy'])  \n","        self.model.count_params()\n","\n","      '''\n","      trains the model and returns a List of various data\n","      data[0] = the train loss after fitting the data \n","      data[1] = the train accuracy after fitting the data  \n","      data[2] = the test loss after evaluating the model \n","      data[3] = the test accuracy after evaluating the model  \n","      '''\n","\n","      def train(self,epoch,datasize,train_images,train_labels,test_images, test_labels):\n","        data = np.zeros(4)\n","        print(datasize,epoch)\n","        history = self.model.fit(train_images[:datasize], train_labels[:datasize],epochs = epoch)\n","        train_loss, train_acc = history.history.values()\n","        data[0] = train_loss[epoch - 1]\n","        data[1] = train_acc[epoch - 1]\n","        test_loss, test_acc = self.model.evaluate(test_images, test_labels, verbose=1)\n","        data[2] = test_loss\n","        data[3] = test_acc\n","        print(self.model.summary())\n","        return data\n","\n","      def getParameter(self):\n","        return self.model.count_params()\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ky-FLTli51-B","executionInfo":{"status":"ok","timestamp":1646564782394,"user_tz":-60,"elapsed":419,"user":{"displayName":"Ritter Rost","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08820756572124446994"}}},"outputs":[],"source":["def trainModel(amount,min_layersize,max_layersize, layers, epochs, datasize, noise):\n","  a = max_layersize - min_layersize \n","  data = np.zeros((5,a))\n","\n","  #load dataset MNIST\n","  mnist = keras.datasets.mnist\n","  #spliting into testing and training\n","  (train_images, train_labels_without_noise), (test_images, test_labels) = mnist.load_data()\n","  #normalizing data\n","  train_images = train_images/255.0\n","  test_images = test_images/255.0\n","  #adding noise\n","  train_labels = adding_noise(train_labels_without_noise, noise)\n","\n","  \n","  '''\n","  data[0][i] = the train loss after fitting the data in iterration i (with i layers) \n","  data[1][i] = the train accuracy after fitting the data in iterration i (with i layers)  \n","  data[2][i] = the test loss after evaluating the model in iterration i (with i layers) \n","  data[3][i] = the test accuracy after evaluating the model in iterration i (with i layers) \n","  data[4][i] = the parameters of the model in iterration i (with i layers)\n","  '''\n","\n","  for i in range(0,max_layersize - min_layersize):\n","    data_help = [0,0,0,0,0]\n","    for j in range(0,amount):\n","      print(\"I an currently in run \",i, \" ===> The model has \", i+ min_layersize ,\"neurons. \")\n","      print(\"amount =\", amount) \n","      myModel = Model(layers,i + min_layersize)\n","      history = myModel.train(epochs, datasize,train_images,train_labels,test_images, test_labels)\n","      param   = myModel.getParameter()\n","      data_help[0] = data_help[0] + history[0] \n","      data_help[1] = data_help[1] + history[1]\n","      data_help[2] = data_help[2] + history[2]\n","      data_help[3] = data_help[3] + history[3]\n","      data_help[4] = data_help[4] + param\n","\n","    data[0][i] = data_help[0]/amount\n","    data[1][i] = data_help[1]/amount\n","    data[2][i] = data_help[2]/amount\n","    data[3][i] = data_help[3]/amount\n","    data[4][i] = data_help[4]\n","  return data     \n","\n","def custom_loss_function(y_true, y_pred):\n","   sum = 0.\n","   for i in range(0,len(y_true)):\n","     tf.autograph.experimental.set_loop_options(\n","        shape_invariants=[(sum, tf.TensorShape(None))]\n","     )\n","     sum = (sum + (y_true[i] - y_pred[i])**2)\n","   return tf.reduce_mean(tf.cast(1/len(y_true),tf.float32)*sum,axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"AZNnw236xlYk"},"source":["#Experimens\n","\n","3 factors semm to have a big impact on the u shaped risk curve \\\n","- The amount of Data. With more training Data the peak of the U-shape-Kurve tend to be more down the x-aches (More neurons are neccecary to get the effekt)\n","- The amount of noice, which was added to the training label. The peak sems to be way higher with higher label noise. \n","- The amount of epochs. All models in this experiment are heaviliy overfitted. However the shape of the U seems to be better visible, if the amount of epochs is very high (around 200)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ue4d3zKo2oY"},"outputs":[],"source":["\n","#experiment on label noise \n","data = []\n","\n","\n","#effect of 30% noise\n","data_20 = trainModel(10,1,100,1,200,4000, 0.3)\n","data.append(data_20)\n","\n","#effect of 15% noise \n","data_10 = trainModel(10,1,100,1,200,4000, 0.15)\n","data.append(data_10)\n","\n","#effect of 0% noise \n","data_0 = trainModel(10,1,100,1,200,4000,0)\n","data.append(data_0)"]},{"cell_type":"markdown","metadata":{"id":"FJeSe7FD4rNO"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O87ezck1hCq1"},"outputs":[],"source":["#plotting the experiment aboce\n","\n","fig3, ax1 = plt.subplots(1, sharex = True)\n","fig3.set_size_inches(20,15)\n","\n","ax1.set_title('kurves with different label noise') \n","ax1.plot(data[0][2],  label= \"30% noise\", color = 'orange')\n","ax1.plot(data[1][2],  label= \"15% noise\",color ='blue')\n","ax1.plot(data[2][2],  label= \"0% noise\", color = 'green')\n","\n","ax1.plot(data[0][0],color = 'orange')\n","ax1.plot(data[1][0],color = 'blue')\n","ax1.plot(data[2][0],color = 'green')\n","\n","\n","#legend\n","ax1.legend(loc='upper right')\n","plt.savefig('label_noise_low.png')"]},{"cell_type":"markdown","metadata":{"id":"1PGriZYP4v_u"},"source":["let's take a look at some higher noise level. \n","We observed, that the Peak of the kurve is higher if the noise is higher. lets find out, what happens if we raise the noise level up to 50%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2ro_gd36z6v"},"outputs":[],"source":["#experiment on label noise \n","data = []\n","\n","#effect of 40% noise\n","data_30 = trainModel(10,1,100,1,200,4000,0.4)\n","data.append(data_30)\n","\n","#effect of 60% noise \n","data_50 = trainModel(10,1,100,1,200,4000,0.6)\n","data.append(data_50)\n","\n","#effect of 80% noise \n","data_70 = trainModel(10,1,100,1,200,4000,0.8)\n","data.append(data_70)\n","\n","fig3, ax1 = plt.subplots(1, sharex = True)\n","fig3.set_size_inches(20,15)\n","\n","ax1.set_title('kurves with different label noise') \n","ax1.plot(data[0][2],  label= \"40% noise\", color = 'green')\n","ax1.plot(data[1][2],  label= \"60% noise\",color ='blue')\n","ax1.plot(data[2][2],  label= \"80% noise\", color = 'orange')\n","\n","ax1.plot(data[0][0],color = 'green')\n","ax1.plot(data[1][0],color = 'blue')\n","ax1.plot(data[2][0],color = 'orange')\n","\n","\n","#legend\n","ax1.legend(loc='upper right')\n","plt.savefig('label_noise-high.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OidZkNx_8qn"},"outputs":[],"source":["#experiment on the effect of epochs\n","\n","data_epochs = []\n","min_epochs = 20\n","max_epochs = 200\n","steps = 40\n","\n","\n","for i in range(min_epochs,max_epochs,steps):\n","  data_epochs.append(trainModel(10,1,100,1,i,4000,0.2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nE2Qq5PO6mZ3"},"outputs":[],"source":["\n","from matplotlib import cm\n","from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n","viridis = cm.get_cmap('viridis', 12)\n","colors = viridis(np.linspace(0, 1, int(((max_epochs - min_epochs)/steps)+1)))\n","\n","#plotting the experiment above\n","\n","fig4, ax1 = plt.subplots(1, sharex = True)\n","fig4.set_size_inches(20,15)\n","\n","ax1.set_title('kurves with different epochs') \n","for i in range(0,len(data_epochs)):\n","  ax1.plot(data_epochs[i][0], color = colors[i], label = str(20 + i*40) + \"epochs\")\n","  ax1.plot(data_epochs[i][2], color = colors[i] )\n","\n","#legend\n","ax1.legend(loc='upper right')\n","plt.savefig('epochs.png')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HryWdhMvCkjG"},"outputs":[],"source":["       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hveWvKPK7dY-"},"outputs":[],"source":["#experiment on the influence of the amount of data\n","data_datasize = []\n","min_datasize = 1000\n","max_datasize = 11000\n","steps = 2000\n","\n","#load dataset MNIST\n","mnist = keras.datasets.mnist\n","#spliting into testing and training\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","#adding noise to the traininglabels, so the effekt is more visible\n","train_labels = adding_noise(train_labels,0.2)\n","\n","for i in range(min_datasize,max_datasize,steps):\n","  data_datasize.append(trainModel(4,1,30,1,100,i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGtWImZOHCIU"},"outputs":[],"source":["\n","from matplotlib import cm\n","from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n","viridis = cm.get_cmap('viridis', 12)\n","colors = viridis(np.linspace(0, 1, int(((max_epochs - min_epochs)/steps)+1)))\n","\n","#plotting the experiment above\n","\n","fig4, ax1 = plt.subplots(1, sharex = True)\n","fig4.set_size_inches(20,15)\n","\n","ax1.set_title('kurves with different datasize') \n","for i in range(0,len(data_datasize)):\n","  ax1.plot(data_datasize[i][1], color = colors[i] )\n","\n","\n","\n","#legend\n","ax1.legend(loc='upper right')\n","plt.savefig('data_size.png')\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Bachelorarbeit.ipynb","provenance":[],"authorship_tag":"ABX9TyPHe8WXJnP57+h9VSR1Ppl0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}